---
title: "StatMethodsHW3B"
author: "Joshua Burkhart"
date: "February 3, 2016"
output: 
  pdf_document: 
    fig_width: 9
    fig_height: 6
    latex_engine: xelatex
---
# BMI 651: HW3 B

```{r global_options, echo=FALSE, include=FALSE, error=FALSE}
knitr::opts_chunk$set(fig.path = "Figs/",
                      message = FALSE,
                      warning = FALSE,
                      include = TRUE,
                      echo = TRUE,
                      error = TRUE,
                      fig.width = 11,
                      comment = NA)
```

```{r, echo=FALSE, include=FALSE}
library(MASS)
library(plyr) #this must be loaded before dplyr 
library(dplyr)
library(ggplot2)
library(broom)
library(knitr)
library(magrittr)
library(reshape2)
library(infotheo)
library(stats)
library(ggbiplot)
library(car)
library("Hiiragi2013")
set.seed(2013) #does this need to be set to 2013? why?
```
  
### Load Data
  
```{r}
data("x")
data("xq")

#feature candidates
hw3B.x.genotypes <- x@phenoData@data$genotype
hw3B.x.probe_intensities <- data.frame(assayDataElement(x@assayData,'exprs'))
hw3B.xq.cell_type <- xq@phenoData@data$Cell.type
hw3B.xq.gene_expressions <- data.frame(assayDataElement(xq@assayData,'exprs'))

#classes
hw3B.x.classes <- x@phenoData@data$Embryonic.day
hw3B.xq.classes <- xq@phenoData@data$Embryonic.day
```

### Scan for missing Data

```{r}
sum(is.na(hw3B.x.genotypes))
sum(is.na(hw3B.x.probe_intensities))
sum(is.na(hw3B.xq.cell_type))
sum(is.na(hw3B.xq.gene_expressions))

sum(is.na(hw3B.x.classes))
sum(is.na(hw3B.xq.classes))
```

```{r}
t(hw3B.xq.gene_expressions) %>% summary()
hw3B.xq.gene_expressions <- na.omit(hw3B.xq.gene_expressions)
```

### Transform positive (E3.25) and negative (E3.5 and E4.5) values 1 and 0, respectively

```{r}
hw3B.x.classes <- as.character(hw3B.x.classes)
hw3B.x.classes[hw3B.x.classes == "E3.25"] = 1
hw3B.x.classes[hw3B.x.classes == "E3.5"] = 0
hw3B.x.classes[hw3B.x.classes == "E4.5"] = 0
hw3B.x.classes <- as.factor(hw3B.x.classes)

hw3B.xq.classes <- as.character(hw3B.xq.classes)
hw3B.xq.classes[hw3B.xq.classes == "E3.25"] = 1
hw3B.xq.classes[hw3B.xq.classes == "E3.5"] = 0
hw3B.xq.classes[hw3B.xq.classes == "E4.5"] = 0
hw3B.xq.classes <- as.factor(hw3B.xq.classes)
```

### Combine Features for x and xq datasets

```{r}
hw3B.x.features <- rbind(hw3B.x.genotypes,hw3B.x.probe_intensities)
hw3B.xq.features <- rbind(as.factor(hw3B.xq.cell_type),hw3B.xq.gene_expressions)
```

### Split into Training & Test sets

```{r}
# x
index <- sample(1:ncol(hw3B.x.features),round(0.8*ncol(hw3B.x.features)))
hw3B.x.features_train <- hw3B.x.features[,index]
hw3B.x.classes_train <- hw3B.x.classes[index]
hw3B.x.features_test <- hw3B.x.features[,-index]
hw3B.x.classes_test <- hw3B.x.classes[-index]

# xq
index <- sample(1:ncol(hw3B.xq.features),round(0.8*ncol(hw3B.xq.features)))
hw3B.xq.features_train <- hw3B.xq.features[,index]
hw3B.xq.classes_train <- hw3B.xq.classes[index]
hw3B.xq.features_test <- hw3B.xq.features[,-index]
hw3B.xq.classes_test <- hw3B.xq.classes[-index]
```

### Z-score training set

```{r}
# x
x.train_sd = matrix()
x.train_mean = matrix()
hw3B.x.features_train_normalized <- hw3B.x.features_train
for(i in 2:nrow(hw3B.x.features_train)) # each row is a feature, first is a factor (genotype)
  x.train_sd[i] = sd(hw3B.x.features_train[i,])
  x.train_mean[i] = apply(hw3B.x.features_train[i,],1,mean)
  for(j in 1:ncol(hw3B.x.features_train)) # each column is a sample
    hw3B.x.features_train_normalized[i,j] =
    (hw3B.x.features_train[i,j] - x.train_mean[i]) / x.train_sd[i]

# xq  
xq.train_sd = matrix()
xq.train_mean = matrix()
hw3B.xq.features_train_normalized <- hw3B.xq.features_train
for(i in 2:nrow(hw3B.xq.features_train)) # each row is a feature, first is a factor (cell_type)
  xq.train_sd[i] = sd(hw3B.xq.features_train[i,])
  xq.train_mean[i] = apply(hw3B.xq.features_train[i,],1,mean)
  for(j in 1:ncol(hw3B.xq.features_train)) # each column is a sample
    hw3B.xq.features_train_normalized[i,j] =
    (hw3B.xq.features_train[i,j] - xq.train_mean[i]) / xq.train_sd[i]
```

> With our datasets now split and Z scored, we are now ready to move on to feature selection (in the next assignment).

## Feature Selection

### Wilcoxan Rank Sum (Univariate Nonparametric Filter)

> First we'll attempt to reduce our prospective features using the wilcoxan rank sum test and a p-value limit of 0.05 for the xq dataset and 0.0001 for the x dataset. We'll store the indices of the features we discover so we can filter any new test data in the same way. The test will retain features whose values for one class are greater or less than those of the other (judged by rank and regardless of distribution).

```{r}
xq.P_LIM = 0.05
x.P_LIM = 0.00000001
x.train_w_idx = vector()
xq.train_w_idx = vector()

#x
for (i in 1:nrow(hw3B.x.features_train_normalized)){
  x.w <- wilcox.test(unlist(hw3B.x.features_train_normalized[i,]) ~ hw3B.x.classes_train,
                     data = hw3B.x.features_train_normalized)
  if(x.w$p.value < x.P_LIM)
    x.train_w_idx <- append(x.train_w_idx,i) #store indices of significant features
  #print(x.train_w_idx %>% length()) debugging
}
print(x.train_w_idx %>% length())

#xq
for (i in 1:nrow(hw3B.xq.features_train_normalized)){
  xq.w <- wilcox.test(unlist(hw3B.xq.features_train_normalized[i,]) ~ hw3B.xq.classes_train,
                     data = hw3B.xq.features_train_normalized)
  if(xq.w$p.value < xq.P_LIM)
    xq.train_w_idx <- append(xq.train_w_idx,i) #store indices of significant features
  #print(xq.train_w_idx %>% length()) debugging 
}
print(xq.train_w_idx %>% length())

#now we can retain only our selected columns
hw3B.xq.features_train_normalized_w <- hw3B.xq.features_train_normalized[xq.train_w_idx,]
hw3B.x.features_train_normalized_w <- hw3B.x.features_train_normalized[x.train_w_idx,]

#add the class labels to the feature data frame
hw3B.xq.net_input <- data.frame(t(hw3B.xq.features_train_normalized_w))
hw3B.x.net_input <- data.frame(t(hw3B.x.features_train_normalized_w))

hw3B.xq.net_input["class"] <- hw3B.xq.classes_train
hw3B.x.net_input["class"] <- hw3B.x.classes_train
```

### Linear Fit (Univariate Parametric Filter)

```{r}
design= model.matrix(hw3B.xq.classes_train)
fit <- lmFit(hw3B.xq.features_train_normalized,hw3B.xq.classes_train)
fit <- eBayes(fit)
topTable(fit,coef=2)

limmaRes = topTable(fit,coef=2,  p.value=0.05, number=500)
print(nrow(limmaRes))

x.ilen <- intersect(limmaRes %>% rownames(),hw3B.xq.net_input %>% colnames()) %>% length()
print(x.ilen / limmaRes %>% nrow())
```

### Bayesian Net Construction (Multivariate Nonparametric Filter)

```{r}
library(bnlearn)
library(Rgraphviz)

bn.xq.hc <- hc(hw3B.xq.net_input)
bn.xq.fast.iamb <- fast.iamb(hw3B.xq.net_input)
bn.x.hc <- hc(hw3B.x.net_input)
# takes too long
#bn.x.fast.iamb <- fast.iamb(hw3B.x.net_input)
```

```{r}
# only show xq because x net is too big
par(mfrow = c(1,2))
graphviz.plot(bn.xq.hc)
graphviz.plot(bn.xq.fast.iamb)
```

### Markov Blanket Filter

```{r}
xq.mhc <- mb(bn.xq.hc,node="class")
xq.mfi <- mb(bn.xq.fast.iamb,node="class")
x.mhc <- mb(bn.x.hc,node="class")
# takes too long
#x.mfi <- mb(bn.x.fast.iamb,node="class")

#use the features common between the greedy hill-climbing and fast iamb blankets
xq.i <- intersect(xq.mhc,xq.mfi)
x.i <- x.mhc #intersect(x.mhc,x.mfi)
```

### SVM

```{r}
library(e1071)

#change to markov blanket features
hw3B.xq.svm_input <- data.frame(t(hw3B.xq.features_train_normalized_w))

#train model
xq.model <- svm(hw3B.xq.svm_input,hw3B.xq.classes_train,kernel="linear")

#check accuracy (on data we just trained on)
xq.class_ag <- table(predict(xq.model,hw3B.xq.svm_input),hw3B.xq.classes_train) %>%
  classAgreement()

print(1 - xq.class_ag$diag) #misclassification rate

#check accuracy (on hold out data)
#normalize
#filter features
#test
```

## References

> [1] http://www.ats.ucla.edu/stat/r/faq/subset_R.htm
  [2] http://r-statistics.co/Statistical-Tests-in-R.html
  [3] http://arxiv.org/pdf/0908.3817.pdf
  [4] http://www.iaeng.org/publication/WCE2010/WCE2010_pp321-328.pdf
  [5] http://ai.stanford.edu/~koller/Papers/Koller+Sahami:ICML96.pdf
