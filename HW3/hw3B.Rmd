---
title: "StatMethodsHW3B"
author: "Joshua Burkhart"
date: "February 3, 2016"
output: 
pdf_document: 
fig_width: 9
fig_height: 6
latex_engine: xelatex
---
# BMI 651: HW3 B

```{r global_options, echo=FALSE, include=FALSE, error=FALSE}
knitr::opts_chunk$set(fig.path = "Figs/",
                      message = FALSE,
                      warning = FALSE,
                      include = TRUE,
                      echo = TRUE,
                      error = TRUE,
                      fig.width = 11,
                      comment = NA)
```

```{r, echo=FALSE, include=FALSE}
library(MASS)
library(plyr) #this must be loaded before dplyr 
library(dplyr)
library(ggplot2)
library(broom)
library(knitr)
library(magrittr)
library(reshape2)
library(infotheo)
library(stats)
library(ggbiplot)
library(car)
library(e1071)
library(bnlearn)
library(Rgraphviz)
library("Hiiragi2013")
set.seed(2013) #does this need to be set to 2013? why?
```

### Load Data

```{r}
data("x")
data("xq")

#feature candidates
hw3B.x.genotypes <- x@phenoData@data$genotype
hw3B.x.probe_intensities <- data.frame(assayDataElement(x@assayData,'exprs'))
hw3B.xq.cell_type <- xq@phenoData@data$Cell.type
hw3B.xq.gene_expressions <- data.frame(assayDataElement(xq@assayData,'exprs'))

#classes
hw3B.x.classes <- x@phenoData@data$Embryonic.day
hw3B.xq.classes <- xq@phenoData@data$Embryonic.day
```

### Scan for missing Data

```{r}
sum(is.na(hw3B.x.genotypes))
sum(is.na(hw3B.x.probe_intensities))
sum(is.na(hw3B.xq.cell_type))
sum(is.na(hw3B.xq.gene_expressions))

sum(is.na(hw3B.x.classes))
sum(is.na(hw3B.xq.classes))
```

```{r}
t(hw3B.xq.gene_expressions) %>% summary()
hw3B.xq.gene_expressions <- na.omit(hw3B.xq.gene_expressions)
```

### Transform positive (E3.25) and negative (E3.5 and E4.5) values 1 and 0, respectively

```{r}
hw3B.x.classes <- as.character(hw3B.x.classes)
hw3B.x.classes[hw3B.x.classes == "E3.25"] = 1
hw3B.x.classes[hw3B.x.classes == "E3.5"] = 0
hw3B.x.classes[hw3B.x.classes == "E4.5"] = 0
hw3B.x.classes <- as.factor(hw3B.x.classes)

hw3B.xq.classes <- as.character(hw3B.xq.classes)
hw3B.xq.classes[hw3B.xq.classes == "E3.25"] = 1
hw3B.xq.classes[hw3B.xq.classes == "E3.5"] = 0
hw3B.xq.classes[hw3B.xq.classes == "E4.5"] = 0
hw3B.xq.classes <- as.factor(hw3B.xq.classes)
```

### Combine Features for x and xq datasets

```{r}
hw3B.x.features <- rbind(hw3B.x.genotypes,hw3B.x.probe_intensities)
hw3B.xq.features <- rbind(as.factor(hw3B.xq.cell_type),hw3B.xq.gene_expressions)
```

### Training & Test Sets

```{r}

### Split with random sampling

# x
index <- sample(1:ncol(hw3B.x.features),round(0.8*ncol(hw3B.x.features)))
hw3B.x.features_train <- hw3B.x.features[,index]
hw3B.x.classes_train <- hw3B.x.classes[index]
hw3B.x.features_test <- hw3B.x.features[,-index]
hw3B.x.classes_test <- hw3B.x.classes[-index]

# xq
index <- sample(1:ncol(hw3B.xq.features),round(0.8*ncol(hw3B.xq.features)))
hw3B.xq.features_train <- hw3B.xq.features[,index]
hw3B.xq.classes_train <- hw3B.xq.classes[index]
hw3B.xq.features_test <- hw3B.xq.features[,-index]
hw3B.xq.classes_test <- hw3B.xq.classes[-index]
```

### Cross Validation Loop

```{r}
# matrices for storing our cv loop result
#          0          1            2                       3                      4         5
#Columns: [feat. sds, feat. means, pct. lm feat. retained, mrkv. blanket indices, err rate, svm]
x.cv_result = matrix()
xq.cv_result = matrix()

#set a large initial error rate and minimize
x.cv_result[4] <- Inf
xq.cv_result[4] <- Inf

for(cv_idx in seq(1:10)){
  
  ### Split Training & Validation sets with random sampling (Monte Carlo cross validation)
  
  # x
  index <-
    sample(1:ncol(hw3B.x.features_train),round(0.9 * ncol(hw3B.x.features_train)))
  hw3B.x.features_train <- hw3B.x.features_train[,index]
  hw3B.x.classes_train <- hw3B.x.classes_train[index]
  hw3B.x.features_validation <- hw3B.x.features_train[,-index]
  hw3B.x.classes_test_validation <- hw3B.x.classes_train[-index]
  
  # xq
  index <-
    sample(1:ncol(hw3B.xq.features_train),round(0.9 * ncol(hw3B.xq.features_train)))
  hw3B.xq.features_train <- hw3B.xq.features_train[,index]
  hw3B.xq.classes_train <- hw3B.xq.classes_train[index]
  hw3B.xq.features_validation <- hw3B.xq.features_train[,-index]
  hw3B.xq.classes_validation <- hw3B.xq.classes_train[-index]
  
  ### Z score training set
  
  # x
  x.train_sd = matrix()
  x.train_mean = matrix()
  hw3B.x.features_train_normalized <- hw3B.x.features_train
  for (i in 2:nrow(hw3B.x.features_train)){
    # each row is a feature, first is a factor (genotype)
    x.train_sd[i] = sd(hw3B.x.features_train[i,])
    x.train_mean[i] = apply(hw3B.x.features_train[i,],1,mean)
    for (j in 1:ncol(hw3B.x.features_train)){
      # each column is a sample
      hw3B.x.features_train_normalized[i,j] =
        (hw3B.x.features_train[i,j] - x.train_mean[i]) / x.train_sd[i]
    }
  }
  
  # xq
  xq.train_sd = matrix()
  xq.train_mean = matrix()
  hw3B.xq.features_train_normalized <- hw3B.xq.features_train
  for (i in 2:nrow(hw3B.xq.features_train)){
    # each row is a feature, first is a factor (cell_type)
    xq.train_sd[i] = sd(hw3B.xq.features_train[i,])
    xq.train_mean[i] = apply(hw3B.xq.features_train[i,],1,mean)
    for (j in 1:ncol(hw3B.xq.features_train)){
      # each column is a sample
      hw3B.xq.features_train_normalized[i,j] =
        (hw3B.xq.features_train[i,j] - xq.train_mean[i]) / xq.train_sd[i]
    }
  }
  
  ## Feature Selection
  
  ### Wilcoxan Rank Sum (Univariate Nonparametric Filter) for x dataset
  
  x.P_LIM = 0.00000001
  x.train_w_idx = vector()
  
  #x
  for (i in 1:nrow(hw3B.x.features_train_normalized)) {
    x.w <-
      wilcox.test(unlist(hw3B.x.features_train_normalized[i,]) ~ hw3B.x.classes_train,
                  data = hw3B.x.features_train_normalized)
    if (x.w$p.value < x.P_LIM)
      x.train_w_idx <-
        append(x.train_w_idx,i) #store indices of significant features
    #print(x.train_w_idx %>% length()) debugging
  }
  #print(x.train_w_idx %>% length())
  
  #now we can retain only our selected columns
  hw3B.x.features_train_normalized_w <-
    hw3B.x.features_train_normalized[x.train_w_idx,]
  
  #add the class labels to the feature data frame
  hw3B.xq.net_input <-
    data.frame(t(hw3B.xq.features_train_normalized))
  hw3B.x.net_input <-
    data.frame(t(hw3B.x.features_train_normalized_w))
  
  hw3B.xq.net_input["class"] <- hw3B.xq.classes_train
  hw3B.x.net_input["class"] <- hw3B.x.classes_train
  
  ### Linear Fit (Univariate Parametric Filter) for x dataset
  
  x.fit <- lmFit(hw3B.x.features_train_normalized,hw3B.x.classes_train)
  x.fit <- eBayes(x.fit)
  topTable(x.fit,coef = 2)
  
  x.limmaRes = topTable(x.fit,coef = 2,  p.value = 0.00000001, number = 500)
  x.ilen <-
    intersect(x.limmaRes %>% rownames(),hw3B.x.net_input %>% colnames()) %>% length()
  
  ### Bayesian Net Construction (Multivariate Nonparametric Filter)
  
  bn.xq.hc <- hc(hw3B.xq.net_input)
  bn.xq.fast.iamb <- fast.iamb(hw3B.xq.net_input)
  bn.x.hc <- hc(hw3B.x.net_input)
  # takes too long
  #bn.x.fast.iamb <- fast.iamb(hw3B.x.net_input)
  
  # show bayesian net for debugging
  #par(mfrow = c(1,2))
  #graphviz.plot(bn.xq.hc)
  #graphviz.plot(bn.xq.fast.iamb)
  
  ### Markov Blanket Filter
  
  # fast.iamb takes too long
  xq.mhc <- mb(bn.xq.hc,node = "class")
  #xq.mfi <- mb(bn.xq.fast.iamb,node="class")
  x.mhc <- mb(bn.x.hc,node = "class")
  #x.mfi <- mb(bn.x.fast.iamb,node="class")
  
  ### SVM
  
  #change to markov blanket features
  hw3B.x.svm_input <-
    data.frame(t(hw3B.x.features_train_normalized_w))
  
  hw3B.xq.svm_input <-
    data.frame(t(hw3B.xq.features_train_normalized))
  
  #train model
  x.model <-
    svm(hw3B.x.svm_input,hw3B.x.classes_train,kernel = "linear")
  
  xq.model <-
    svm(hw3B.xq.svm_input,hw3B.xq.classes_train,kernel = "linear")
  
  ### Validation (on hold out data)

  #Z score validation data using previously calculated means & standard deviations
  
  # x
  hw3B.x.features_validation_normalized <- hw3B.x.features_validation
  for (i in 2:nrow(hw3B.x.features_validation)){
    # each row is a feature, first is a factor (genotype)
    for (j in 1:ncol(hw3B.x.features_validation)){
      # each column is a sample
      hw3B.x.features_validation_normalized[i,j] =
        (hw3B.x.features_validation[i,j] - x.train_mean[i]) / x.train_sd[i]
    }
  }
  
  # xq
  hw3B.xq.features_validation_normalized <- hw3B.xq.features_validation
  for (i in 2:nrow(hw3B.xq.features_validation)){
    # each row is a feature, first is a factor (cell_type)
    for (j in 1:ncol(hw3B.xq.features_validation)){
      # each column is a sample
      hw3B.xq.features_validation_normalized[i,j] =
        (hw3B.xq.features_validation[i,j] - xq.train_mean[i]) / xq.train_sd[i]
    }
  }
  
  #filter features
  hw3B.x.svm_input <- base::subset(hw3B.x.features_validation_normalized,select=x.mhc)
  hw3B.xq.svm_input <- base::subset(hw3B.xq.features_validation_normalized,select=xq.mhc)
  
  #test
  x.class_ag <-
    table(predict(x.model,hw3B.x.svm_input),hw3B.x.classes_validation) %>%
    classAgreement()
  
  xq.class_ag <-
    table(predict(xq.model,hw3B.xq.svm_input),hw3B.xq.classes_validation) %>%
    classAgreement()
  
  #misclassification rate
  x.err_rate <- 1 - x.class_ag$diag
  xq.err_rate <- 1 - xq.class_ag$diag
  
  if(x.err_rate < x.cv_result[4]){
    x.cv_result[0] <- x.train_sd
    x.cv_result[1] <- x.train_mean
    x.cv_result[2] <- x.ilen / (x.limmaRes %>% nrow())
    x.cv_result[3] <- x.mhc
    x.cv_result[4] <- x.err_rate
  }
  
  if(xq.err_rate < xq.cv_result[4]){
    xq.cv_result[0] <- xq.train_sd
    xq.cv_result[1] <- xq.train_mean
    xq.cv_result[2] <- NA
    xq.cv_result[3] <- xq.mhc
    xq.cv_result[4] <- xq.err_rate
    xq.cv_result[5] <- xq.model
  }
}

print(x.cv_result)
print(xq.cv_result)
```
