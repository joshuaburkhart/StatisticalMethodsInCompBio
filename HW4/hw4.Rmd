---
title: "StatMethodsHW4"
author: "Joshua Burkhart"
date: "February 18, 2016"
output: 
pdf_document: 
fig_width: 9
fig_height: 6
latex_engine: xelatex
---
# BMI 651: HW4

```{r global_options, echo=FALSE, include=FALSE, error=FALSE}
knitr::opts_chunk$set(fig.path = "Figs/",
                      message = FALSE,
                      warning = FALSE,
                      include = TRUE,
                      echo = TRUE,
                      error = TRUE,
                      fig.width = 11,
                      comment = NA)
```

```{r, echo=FALSE, include=FALSE}
library(MASS)
library(plyr) #this must be loaded before dplyr 
library(dplyr)
library(ggplot2)
library(broom)
library(knitr)
library(magrittr)
library(tree)
library(rpart)
library(adabag)
set.seed(1234)
```

```{r}
spam.df <- read.table("~/SoftwareProjects/StatisticalMethodsInCompBio/HW4/spam.data")
```

##1

> Our dataset contains 1813 rows labelled 1 (spam) and 2788 rows labelled 0 (not spam) for a 60.6% (non spam) to 39.4% (spam) split. As a constant (naive) classifier could achieve a 39.4% misclassification rate by ouputting 0 (non spam) for each example, we'll hope to develop a classifier with a lower misclassification rate.

```{r}
spam.df %>% dim()
spam.df[,58] %>% table()
spam.df[,58] %>% table() %>% as.vector() %>% .[1]/spam.df %>% nrow()
spam.df[,58] %>% table() %>% as.vector() %>% .[2]/spam.df %>% nrow()
```

##2

> Split data (1:1 split)

```{r}
index <- sample(1:nrow(spam.df),round(0.5*nrow(spam.df)))
spam.df_train <- spam.df[index,]
spam.df_test <- spam.df[-index,]
```

> Assure training & test datasets are non-overlapping. Interestingly, the original dataset appears to have 391 duplicated rows. We'll filter these out and resplit our data into training and test datasets. We'll also check the class (spam / non spam) distribution of the filtered spam dataset and observe it changes only slightly to 60.1% non spam, 39.9% spam. Finally, we see that our training and test datasets contain no intersecting (overlapping) rows.

```{r}
duplicated(spam.df) %>% table()

spam.df_unique <- unique(spam.df)

spam.df_unique %>% dim()
spam.df_unique[,58] %>% table()
spam.df_unique[,58] %>% table() %>% as.vector() %>% .[1]/spam.df_unique %>% nrow()
spam.df_unique[,58] %>% table() %>% as.vector() %>% .[2]/spam.df_unique %>% nrow()

index <- sample(1:nrow(spam.df_unique),round(0.5*nrow(spam.df_unique)))
spam.df_train <- spam.df_unique[index,]
spam.df_test <- spam.df_unique[-index,]

intersect(spam.df_train,spam.df_test) %>% nrow()
```

> Assure split requested (1:1) is correct

```{r}
spam.df_train %>% dim()
spam.df_test %>% dim()
```

> Compare percentage of spam in training & test datasets

```{r}
# training set
spam.df_train[,58] %>% table()
spam.df_train[,58] %>% table() %>% as.vector() %>% .[1]/spam.df_train %>% nrow()
spam.df_train[,58] %>% table() %>% as.vector() %>% .[2]/spam.df_train %>% nrow()

# test set
spam.df_test[,58] %>% table()
spam.df_test[,58] %>% table() %>% as.vector() %>% .[1]/spam.df_test %>% nrow()
spam.df_test[,58] %>% table() %>% as.vector() %>% .[2]/spam.df_test %>% nrow()
```

##3

> Build classification tree for training dataset pruned following cross validation. Interestingly, K=10 fold cross validation shows deviance decreasing for each additional split, up to 11 (for a tree with 12 terminal nodes), the same number of splits used to build the original tree. Thus, nothing is actually pruned. That the tree stops after 11 splits is likely an artifact of the built-in heuristics of the R 'tree' library.

```{r}
# applying factor() to the response forces production of a classification tree
# (Brian, A., & Ripley, M. B. (2016). Package “ tree .”)
spam.tree <- tree(factor(spam.df_train$V58)~.,data=spam.df_train)

# perform K=10 fold cross validation
spam.tree_cv <- cv.tree(spam.tree,K=10)

# prune the tree, allowing 12 terminal nodes
spam.tree_pruned <- prune.tree(spam.tree,best=12)
```

> Generate plots for cv error vs tree size. The deviance decreases for each of the 11 splits (resulting in 12 terminal nodes).

```{r}
plot(spam.tree_cv,main="Tree Size vs Deviance (Number of Misclassifications)\n\n")
```

> Plotting best tree

```{r}
plot(spam.tree_pruned,main="'Pruned' Tree\n")
text(spam.tree_pruned)
```

> Getting misclassification error on test dataset. Using only 11 splits, we were able to achieve a 10.1% misclassification rate using our 'pruned' tree.

```{r}
# perform prediction on test dataset
spam.test_pred <- predict(spam.tree_pruned,spam.df_test[,-58],type="class")

# calculate misclassification percentage
table(as.integer(as.character(spam.test_pred)) == spam.df_test[,58]) %>% .[1] / length(spam.test_pred)
```

> Summarize variables in 'pruned' tree

```{r}
spam.tree_pruned %>% summary()
```

##4

> Bagging for ensemble of 100 trees for training data

```{r}
spam.df_train$V58 <- factor(spam.df_train$V58)
spam.bagging <- bagging(V58~.,data=spam.df_train,mfinal=100)
```

> plot of the importance of the variables, (for ensemble)

```{r}
imp <- data.frame(spam.bagging$importance)
ggplot(data=imp, aes(x=rownames(imp),y=imp$spam.bagging.importance)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Variable") +
  ylab("Importance") +
  ggtitle("Bagging Variable Importance")
```

> Calculating test error rate... we see a slight reduction in test error after bagging, 10.1% to 9.36% error, a reduction of 0.74%.

```{r}
spam.df_test$V58 <- factor(spam.df_test$V58)
spam.bagging_test_pred <- predict(spam.bagging,spam.df_test,type="class")
spam.bagging_test_pred$error
```

##5

> Boosting for ensemble of 100 trees for training data

```{r}
spam.boosting <- boosting(V58~.,data=spam.df_train,mfinal=100)
```

> plot of the importance of the variables, (for ensemble)

```{r}
imp <- data.frame(spam.boosting$importance)
ggplot(data=imp, aes(x=rownames(imp),y=imp$spam.boosting.importance)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Variable") +
  ylab("Importance") +
  ggtitle("Boosting Variable Importance")
```

> Calculating test error rate... we see our test error rate nearly cut in half after boosting, 9.36% to 5.13% error, a reduction of 4.23%.

```{r}
spam.boosting_test_pred <- predict(spam.boosting,spam.df_test,type="class")
spam.boosting_test_pred$error
```
